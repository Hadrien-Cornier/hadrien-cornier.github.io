



in the game turing complete the computer happens when the memory can influence compute and the compute can influence the memory. The dual interation unlocks all of programming.


In the JEPA paper of Yann Le Cun, Mode 2 perception is just a recurrent neural network, The idea of time and persisting mental models through time is what allows us human brains to rise above the animal that can only act by instinct. Acting by instinct is just a recurrent neural network with a very small memory capacity
Acting with a view towards future rewards is what Yann le Cun is getting at but this is achieved in reality by having a larger context size


What i suspect is that system 1 and system 2 thinking are just transformers with a smaller or bigger context size

Emotions are a form of embedding, A distillation of knowledge into a compressed form that can be parsed at once

System 2 thinking only allows you to allocate a finite amount of things in your mental space to make connections in between of. Which means we have a limited memory stack when we try to solve complex mental problems. And the capacity to hold bigger and bigger representations of systems in memory is what allows us to be more intelligent when we optimize for system2 future expected rewards. Essentially this is why "longest number sequence you can memorize in one-shot" is closely related to factor $g$ the general intelligence. It essentially represents the bandwidth of a computer


George Hotz sees the human machine intelligence question as a continuum. There is a computer in everyone of us and that computer runs on a flesh and bone biological stack instead of in a computer silicon stack. The stack is everything from the perception to the decision of action. 

So in other words we reach higher and higher intelligence states by simply retaining more context. 
For instance if we were able to process every previous impression of a job seeker along with the newest feature set at every point in the inference process, then we should logically be able to make even better decisions by reaching "system2" levels of cognition. 



Self supervised learning is more sample efficient than single label prediction


Can we do self-supervised training on our categorical rows ? 


Ie reconstruct the masked features ? 
This will be a pretext task to simply pretrain the network, just like BERT does 
 



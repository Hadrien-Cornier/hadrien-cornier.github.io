https://arxiv.org/abs/2112.05682


Basically they do the normalization for the softmax in a different way using a simple rewrite of the Attention formula


It only requires O(log(n)) memory now !